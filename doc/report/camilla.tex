\subsection{Embeddings}
Among the approaches we decided to implement, we opted for an SVM classifier trained using embeddings-based textual representations. 

We first tested three different representations based on the same lexicon of  precomputed static word embeddings, the Italian Twitter Embeddings \cite{italian_twitter_embeddings} by the ItaliaNLP Lab, differing by method of aggregation. 

In order to do so, we loaded the embeddings and our preprocessed and annotated training documents, which we further normalized using the functions provided by the ItaliaNLP lab. We could then build the vocabulary of the training set using the embeddings. Given the characteristics of this type of non-standard language, in particular the high level of both inter and intrapersonal variation, we expected some out-of-vocabulary words, which ended up being 2703 out of 22709, meaning that almost 12\% of words didn't have an embedding representation. These words are mostly words that haven't been correctly tokenized (especially multi-words hashtags), sequences of emojis, and typos. Had we used non-specialized embeddings, we assume that this coverage would have been significantly lower, posing an important issue for the model training.

Using a 5-fold cross-validation process and comparing the resulting macro-averaged F1 score, we determined the best performing model to be the one representing each tweet with the average of all its word embeddings. It performed better than using the aggregation of only lexically-full tokens (nouns, verbs and adjectives), both by mean and by concatenation of the mean vectors for the three aforementioned grammatical categories.

\begin{table}
\caption{SVM classifiers based on different word embeddings aggregations (5-fold cross-validation).}
    \begin{tabular}{rlr}
        \toprule
        & \textsc{Features} & \textsc{Macro F1} \\
        \midrule
        1 & Avg over all tokens & 0.744 \\
        2 & Avg over all N, V and Adj & 0.726 \\
        3 & Concat. of mean vects (N, V, Adj) & 0.717 \\
        \bottomrule
    \end{tabular}
    \label{tbl:svm_f1_static_embs_aggregations}
\end{table}

We then followed the same methodology to test representations based on contextual embeddings extracted from two different Italian pre-trained BERT models.
\begin{enumerate}
    \item \textbf{dbmdz/bert-base-italian-cased} or (\textbf{Italian BERT}) \cite{italian_bert}:  trained on a corpus consisting of a Wikipedia dump and texts from the OPUS corpora collection, for a total of 2,050,057,573 tokens.
    \item \texttt{\textbf{m\_polignano\_uniba/bert\_uncased\_L{-}12\_H{-}\allowbreak768\_A{-}12\_italian\_alb3rt0}} (or \textbf{AlBERTo}) \cite{alberto}:  the first Italian BERT model for Twitter language understanding, trained on Italian tweets.
\end{enumerate}

For both models, we tested two methods of \textit{pooling} the tokens' embedding representations generated by the model after processing the input sequence into a single sentence embedding for each tweet (or, more generally, document).
\begin{enumerate}
    \item \textbf{[CLS] pooling}: the model's tokenizer adds the special token [CLS] at the start of every sequence. During the training process of the Large Language Model, its embedding (or hidden state) will capture sequence-level information. By extracting the [CLS] token's embedding of each document from the final hidden layer, we obtain a representation that can serve as a sentence-level embedding.
    \item \textbf{Mean Pooling with Sentence Transformers}: we used a Sentence Transformer, a model that generates a single fixed-length vector representation for an entire sequence. It does so via \textit{mean pooling}, i.e. by computing the mean of all tokens' embeddings in the last hidden layer.
\end{enumerate}

The Italian BERT yielded worse performances, which can be explained by the fact that it is a general model that has not been trained for this specific domain. AlBERTo, on the other hand, provided better results, with the SVM model based on its mean pooled embeddings having the highest mean macro F1 score.
    
\begin{table}
\caption{Mean macro F1 score of SVM classifiers based on different contextual embeddings' poolings (5-fold cross-validation).}
    \begin{tabular}{lrr}
        \toprule
        & \textsc{[CLS]} & \textsc{Mean Pooling} \\
        \midrule
        Italian BERT & 0.689 & 0.725 \\
        AlBERTo & 0.741 & 0.747 \\
        \bottomrule
    \end{tabular}
    \label{tbl:svm_f1_contextual_embs_pooling}
\end{table}

Once we decreed our best performing model, we proceeded to test it on our two test sets, bearing in mind that it is a relatively simple model, using only the text itself as a feature.

\begin{table}[h]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \midrule
        0 & 0.741 & 0.718 & 0.729 & 641 \\
        1 & 0.718 & 0.741 & 0.729 & 622 \\
        \midrule
        \textbf{accuracy} & & & 0.729 & 1263 \\
        \textbf{macro avg} & 0.729 & 0.729 & 0.729 & 1263 \\
        \textbf{weighted avg} & 0.730 & 0.729 & 0.729 & 1263 \\
        \bottomrule
    \end{tabular}
    \caption{Classification Report on Tweets test set}
    \label{tab:classification_report_svm_alberto_tweets}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        & \textbf{precision} & \textbf{recall} & \textbf{f1-score} & \textbf{support} \\
        \midrule
        0 & 0.730 & 0.881 & 0.798 & 319 \\
        1 & 0.670 & 0.425 & 0.520 & 181 \\
        \midrule
        \textbf{accuracy} & & & 0.716 & 500 \\
        \textbf{macro avg} & 0.700 & 0.653 & 0.659 & 500 \\
        \textbf{weighted avg} & 0.708 & 0.716 & 0.698 & 500 \\
        \bottomrule
    \end{tabular}
    \caption{Classification Report on News headlines test set}
    \label{tab:classification_report_svm_alberto_news}
\end{table}
The macro F1 drops from 0.747 to 0.729 in the in-domain task, but all metrics are balanced between the two classes. The model seems to be able to generalize well on tweets and has a stable performance.

As expected, the performance drop is more significant for the out-of-domain task, with a macro F1 of 0.659. The model performs worse for class 1, especially with respect to F1 and recall, which means that it struggles with detecting hate speech in news headlines.
