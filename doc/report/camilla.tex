\subsection{Embeddings}
\label{subsec:Embeddings}
Among the approaches we decided to implement, we opted for an SVM classifier trained using embeddings-based textual representations. 

We first tested three different representations based on the same lexicon of  precomputed static word embeddings, the Italian Twitter Embeddings \cite{italian_twitter_embeddings} by the ItaliaNLP Lab, differing by method of aggregation. 

In order to do so, we loaded the embeddings and our preprocessed and annotated training documents, which we further normalized using the functions provided by the ItaliaNLP lab. We could then build the vocabulary of the training set using the embeddings. Given the characteristics of this type of non-standard language, in particular the high level of both inter and intrapersonal variation, we expected some out-of-vocabulary words, which ended up being 2703 out of 22709, meaning that almost 12\% of words didn't have an embedding representation. These words are mostly words that haven't been correctly tokenized (especially multi-words hashtags), sequences of emojis, and typos. Had we used non-specialized embeddings, we assume that this coverage would have been significantly lower, posing an important issue for the model training.

Using a 5-fold cross-validation process and comparing the resulting macro-averaged F1 score, we determined the best performing model to be the one representing each tweet with the average of all its word embeddings. It performed better than using the aggregation of only lexically-full tokens (nouns, verbs and adjectives), both by mean and by concatenation of the mean vectors for the three aforementioned grammatical categories.

\begin{table}
    \begin{tabular}{rlr}
        \toprule
        & \textsc{Features} & \textsc{Macro F1} \\
        \midrule
        1 & Avg over all tokens & 0.744 \\
        2 & Avg over all N, V and Adj & 0.726 \\
        3 & Concat. of mean vects (N, V, Adj) & 0.717 \\
        \bottomrule
    \end{tabular}
    \caption{SVM classifiers based on different word embeddings aggregations (5-fold cross-validation).}
    \label{tbl:svm_f1_static_embs_aggregations}
\end{table}

We then followed the same methodology to test representations based on contextual embeddings extracted from two different Italian pre-trained BERT models. \footnote{\textbf{BERT} (Bidirectional Encoder Representations from Transformers) is a bidirectional Large Language Model (LLM) based on an encoder-only transformer architecture. This Deep Learning model was presented by Google in 2018.}
\begin{enumerate}
    \item \textbf{dbmdz/bert-base-italian-cased} or (\textbf{Italian BERT}) \cite{italian_bert}:  trained on a corpus consisting of a Wikipedia dump and texts from the OPUS corpora collection, for a total of 2,050,057,573 tokens.
    \item \texttt{\textbf{m\_polignano\_uniba/bert\_uncased\_L{-}12\_H{-}\allowbreak768\_A{-}12\_italian\_alb3rt0}} (or \textbf{AlBERTo}) \cite{alberto}:  the first Italian BERT model for Twitter language understanding, trained on Italian tweets.
\end{enumerate}

For both models, we tested two methods of \textit{pooling} the tokens' embedding representations generated by the model after processing the input sequence into a single sentence embedding for each tweet (or, more generally, document).
\begin{enumerate}
    \item \textbf{[CLS] pooling}: the model's tokenizer adds the special token [CLS] at the start of every sequence. During the training process of the Large Language Model, its embedding (or hidden state) will capture sequence-level information. By extracting the [CLS] token's embedding of each document from the final hidden layer, we obtain a representation that can serve as a sentence-level embedding.
    \item \textbf{Mean Pooling with Sentence Transformers}: we used a Sentence Transformer, a model that generates a single fixed-length vector representation for an entire sequence. It does so via \textit{mean pooling}, i.e. by computing the mean of all tokens' embeddings in the last hidden layer.
\end{enumerate}

The Italian BERT yielded worse performances, which can be explained by the fact that it is a general model that has not been trained for this specific domain. AlBERTo, on the other hand, provided better results, with the SVM model based on its mean pooled embeddings having the highest mean macro F1 score.
    
\begin{table}
\centering
    \begin{tabular}{lrr}
        \toprule
        & \textsc{[CLS]} & \textsc{Mean Pooling} \\
        \midrule
        Italian BERT & 0.689 & 0.725 \\
        AlBERTo & 0.741 & 0.747 \\
        \bottomrule
    \end{tabular}
    \caption{Mean macro F1 score of SVM classifiers based on different contextual embeddings' poolings (5-fold cross-validation).}
    \label{tbl:svm_f1_contextual_embs_pooling}
\end{table}
