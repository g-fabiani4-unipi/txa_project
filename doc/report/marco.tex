\subsection{Ngrams extraction and modelling}
In this section, we describe the process used to extract n-grams from tweets to exploit them as features.
The process begins with a pre-processing step applied to the raw training dataset, implemented through the \texttt{clean\textunderscore df} function, which consists of three inner functions.

The first inner function, \texttt{count\textunderscore uppercase}, calculates the number of uppercase characters in the dataset.
When we considered the stylistic features, we successfully replaced this simple counter with a function that computes ratio of all uppercase words in a document.

Additionally, we defined a custom punctuation set that extends the standard \texttt{string.punctuation} by including special characters encountered in tweets.
After applying this function, the dataset includes the original columns, \texttt{text} and \texttt{hs}, along with a new column, \texttt{uppercase\textunderscore count}.

The second inner function, \texttt{split\textunderscore hashtags}, utilizes the WordSegment library to split words within hashtags. This function is stochastic, meaning it may split hashtags differently during each execution. Based on our observations, the function performs well in identifying and splitting longer words within hashtags but struggles with stopwords and shorter words. This second inner function is incorporated into the \texttt{process\textunderscore hashtags} function, which tokenizes hashtags when more than one word is present. An additional condition was added to handle words longer than 10 characters, although this could occasionally split valid long words incorrectly. However, given the rarity of such words in the dataset, this approach is effective for identifying incorrectly joined words.
At this stage, the dataset includes a new column, \texttt{text\textunderscore processed}.

Subsequently, all text is converted to lowercase, mentions, URLs, and numbers are removed, and the tweets are tokenized using \emph{Stanza} \cite{stanza}.
Tokens are then further processed by removing stopwords and punctuation.
After tokenization, two additional columns are added: one reporting the final number of tokens per tweet and another counting the occurrence of bad words, if any.
The bad words were identified using a list of 500 Italian bad words retrieved from GitHub\footnote{\url{https://github.com/napolux/paroleitaliane/tree/master/paroleitaliane}}.

Once the tokenized version of the tweets was prepared, we lemmatized the tokens to ensure higher consistency and reduce the number of final n-grams. Given the brevity of tweets, we limited the n-grams to unigrams and bigrams.

The final dataset consisted of 6837 rows and 6366 features, including the original tweet, tokenized version, lemmatized version, the number of uppercase characters, the count of bad words, and extracted unigrams and bigrams. Before extracting additional features and embeddings, we conducted initial experiments by training models directly on this dataset. Several models were tested, including Random Forest, SVM, XGBoost, and Logistic Regression.
After applying random search for hyperparameter optimization, the best performance was achieved with XGBoost, yielding a macro average F1 score of 0.759.

The same preprocessing steps were applied to both test sets: one containing tweets and the other consisting of 500 headlines from Italian newspapers.
To ensure consistency between the test datasets and the training set, we used the dictionary, the Bigram model, and the TF-IDF model trained on the training set.
As a result, the final tweets test dataset has the same number of features as the training set, minus one (the target class). This ensures that the model trained on the training dataset can be correctly applied to the test set.
Specifically, if an n-gram is present in both the training and test sets, the corresponding column will report non-zero values; otherwise, the column will consist entirely of zeros.
Additionally, by using the same TF-IDF model for both training and testing, the weight of each n-gram remains consistent.
The final dataset consisted of 6837 rows and 6366 features, including the original tweet, tokenized version, lemmatized version, the number of uppercase characters, the count of bad words, and extracted unigrams and bigrams.
Before extracting additional features and embeddings, we conducted initial experiments by training models directly on this dataset. Several models were tested, including Random Forest, SVM, XGBoost, and Logistic Regression. After applying random search for hyperparameter optimization, the best performance was achieved with XGBoost, yielding a macro average F1 score of 0.759.