{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to reproduce the methodology used for the original collection of the documents, so we filter the headlines searching for those containing mentions of immigrants, muslims and Roma. We use the same set of neutral(?) keywords used originally by Poletto et al. 2017\n",
    "\n",
    "ethnic group  | religion    | Roma\n",
    "--------------|-------------|------\n",
    "immigrat*     | terrorismo  | rom\n",
    "immigrazione  | terrorist*  | nomad*\n",
    "migrant*      | islam       |\n",
    "stranier*     | mussulman*  |\n",
    "profug*       | corano      |\n",
    "\n",
    "We add also the frequently used variant \"musulman*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = re.compile(r\"immigrat.|immigrazione|migrant.|stranier.|profug.|terroris(mo|t.)|islam|mus(s)?ulman.|corano|\\Wrom\\W|nomad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headlines in the test set come from the online editions of:\n",
    "- La Stampa\n",
    "- La Repubblica\n",
    "- Il Giornale\n",
    "- Liberoquotidiano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Stampa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all titles with containing the keywords from the cronaca, politica and esteri sections of \"La Stampa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing cronaca/\n",
      "processing politica/\n",
      "processing esteri/\n"
     ]
    }
   ],
   "source": [
    "sections = [\"cronaca/\", \"politica/\", \"esteri/\"]\n",
    "base_url = \"https://www.lastampa.it/\"\n",
    "titles_lastampa = []\n",
    "\n",
    "for section in sections:\n",
    "    max_pages = 10000\n",
    "    page = 0\n",
    "    print(\"processing\", section)\n",
    "    while page < max_pages:\n",
    "        page += 1\n",
    "        page_url = base_url + section + f\"{page}/\"\n",
    "        response = requests.get(page_url)\n",
    "        if response.status_code != 200:\n",
    "            print(page_url, response.status_code)\n",
    "            continue\n",
    "        source = BeautifulSoup(response.text)\n",
    "        if page == 1:\n",
    "            max_pages = int(source.find(class_=\"pagination__counter\").text.split()[-1])\n",
    "        entry_titles = source.find_all(class_=\"entry__title\")\n",
    "        for t in entry_titles:\n",
    "            text = t.a.get_text(strip=True)\n",
    "            if keywords.search(text.lower()):\n",
    "                titles_lastampa.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 headlines from La Stampa\n"
     ]
    }
   ],
   "source": [
    "print(len(titles_lastampa), \"headlines from La Stampa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Repubblica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing cronaca/\n",
      "processing politica/\n",
      "processing esteri/\n"
     ]
    }
   ],
   "source": [
    "sections = [\"cronaca/\", \"politica/\", \"esteri/\"]\n",
    "base_url = \"https://www.repubblica.it/\"\n",
    "titles_repubblica = []\n",
    "\n",
    "for section in sections:\n",
    "    max_pages = 10000\n",
    "    page = 0\n",
    "    print(\"processing\", section)\n",
    "    while page < max_pages:\n",
    "        page += 1\n",
    "        page_url = base_url + section + f\"{page}/\"\n",
    "        response = requests.get(page_url)\n",
    "        if response.status_code != 200:\n",
    "            print(page_url, response.status_code)\n",
    "            continue\n",
    "        source = BeautifulSoup(response.text)\n",
    "        if page == 1:\n",
    "            max_pages = int(source.find(class_=\"pagination__counter\").text.split()[-1])\n",
    "        articles = source.find_all(\"article\", class_=\"type-articolo\")\n",
    "        for art in articles:\n",
    "            text = art.h2.a.get_text(strip=True)\n",
    "            if keywords.search(text.lower()):\n",
    "                titles_repubblica.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles_repubblica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Il Giornale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing interni.html\n",
      "processing cronache.html\n",
      "processing esteri.html\n"
     ]
    }
   ],
   "source": [
    "sections = [\"interni.html\", \"cronache.html\", \"esteri.html\"]\n",
    "base_url = \"https://www.ilgiornale.it/sezioni/\"\n",
    "titles_ilgiornale = []\n",
    "\n",
    "for section in sections:\n",
    "    print(\"processing\", section)\n",
    "    page = 1\n",
    "    while page < 400:\n",
    "        response = requests.get(base_url + section, params={'page': page})\n",
    "        # Break the cicle if the page was not found\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        source = BeautifulSoup(response.text)\n",
    "        entry_titles = source.find_all(class_=\"card__title\")\n",
    "        for t in entry_titles:\n",
    "            text = t.get_text(strip=True)\n",
    "            if keywords.search(text.lower()):\n",
    "                titles_ilgiornale.append(text)\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles_ilgiornale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liberoquotidiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing politica/\n",
      "processing giustizia/\n",
      "processing italia/\n",
      "processing europa/\n",
      "processing esteri/\n",
      "processing piulibero/\n",
      "processing terra-promessa\n"
     ]
    }
   ],
   "source": [
    "sections = [\"politica/\", \"giustizia/\", \"italia/\", \"europa/\", \"esteri/\", \"piulibero/\", \"terra-promessa\"]\n",
    "base_url = \"https://www.liberoquotidiano.it/\"\n",
    "titles_libero = []\n",
    "\n",
    "for section in sections:\n",
    "    page = 1\n",
    "    print(\"processing\", section)\n",
    "    while True:\n",
    "        page_url = base_url + section + f\"page/{page}/\"\n",
    "        response = requests.get(page_url)\n",
    "        # Break the cicle if the page was not found\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        source = BeautifulSoup(response.text)\n",
    "        articles = source.main.find_all('article')\n",
    "\n",
    "        for art in articles:\n",
    "            text = art.header.h2.get_text(strip=True)\n",
    "            if keywords.search(text.lower()):\n",
    "                titles_libero.append(text)\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles_libero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "documents = titles_lastampa + titles_repubblica + titles_ilgiornale + titles_libero\n",
    "old_documents = []\n",
    "data_dir = Path('../data')\n",
    "\n",
    "with open(data_dir/ 'headlines.txt', 'r', encoding='utf-8') as infile:\n",
    "    for line in infile.readlines():\n",
    "        old_documents.append(line.strip())\n",
    "\n",
    "documents = list(set(old_documents + documents))\n",
    "\n",
    "with open(data_dir / 'headlines.txt', 'w', encoding='utf-8') as outfile:\n",
    "    for document in documents:\n",
    "        outfile.write(document + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txa_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
